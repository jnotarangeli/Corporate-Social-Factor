{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8885c69",
   "metadata": {},
   "source": [
    "# Fortune / Great Place to Work â€œ100 Bestâ€ Lists â†’ CSV (Beautiful Soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75750b8b",
   "metadata": {},
   "source": [
    "###### Scalling up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e8ba97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -------------------------\n",
    "# 0) Helpers\n",
    "# -------------------------\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "def gvkey6(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    s = str(x).strip()\n",
    "    s = \"\".join(ch for ch in s if ch.isdigit())\n",
    "    return s.zfill(6) if s else pd.NA\n",
    "\n",
    "def make_match_key(name: str) -> str:\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    n = name.lower()\n",
    "    n = re.sub(r\"\\(.*?\\)\", \"\", n)      # remove parentheses\n",
    "    n = n.replace(\"&\", \"and\")\n",
    "    n = re.sub(r\"[^a-z0-9\\s]\", \" \", n) # remove punctuation\n",
    "    suffixes = {\n",
    "        \"inc\", \"incorporated\", \"corp\", \"corporation\", \"co\", \"company\",\n",
    "        \"llc\", \"llp\", \"lp\", \"plc\", \"ltd\", \"limited\",\n",
    "        \"group\", \"holdings\"\n",
    "    }\n",
    "    tokens = [t for t in n.split() if t and t not in suffixes]\n",
    "    return \" \".join(tokens).strip()\n",
    "\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def parse_rank_company_pairs_from_text(soup: BeautifulSoup):\n",
    "    raw_lines = [normalize_whitespace(x) for x in soup.get_text(\"\\n\").splitlines()]\n",
    "    lines = [x for x in raw_lines if x]\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    while i < len(lines) - 1:\n",
    "        if re.fullmatch(r\"\\d{1,3}\", lines[i]):\n",
    "            rank = int(lines[i])\n",
    "            name = lines[i + 1].strip()\n",
    "            if 1 <= rank <= 100 and not re.fullmatch(r\"\\d{1,3}\", name):\n",
    "                results.append((rank, name))\n",
    "                i += 2\n",
    "                continue\n",
    "        i += 1\n",
    "\n",
    "    # keep first company seen per rank\n",
    "    dedup = {}\n",
    "    for r, n in results:\n",
    "        if r not in dedup:\n",
    "            dedup[r] = n\n",
    "    return sorted(dedup.items(), key=lambda x: x[0])\n",
    "\n",
    "def scrape_gptw_from_url(url: str, year: int | None = None, sleep_s: float = 0.75) -> pd.DataFrame:\n",
    "    if year is None:\n",
    "        m = re.search(r\"/(\\d{4})(?:/)?$\", url)\n",
    "        if not m:\n",
    "            raise ValueError(\"Could not infer year from URL; pass year=YYYY explicitly.\")\n",
    "        year = int(m.group(1))\n",
    "\n",
    "    time.sleep(sleep_s)\n",
    "    html = requests.get(url, headers=DEFAULT_HEADERS, timeout=30).text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    pairs = parse_rank_company_pairs_from_text(soup)\n",
    "\n",
    "    df_raw = pd.DataFrame(pairs, columns=[\"rank\", \"company_name\"])\n",
    "    df_raw.insert(0, \"year\", year)\n",
    "    return df_raw\n",
    "\n",
    "# -------------------------\n",
    "# 1) Build a Compustat name index ONCE\n",
    "# -------------------------\n",
    "\n",
    "def build_compustat_name_pool(db):\n",
    "    \"\"\"\n",
    "    Pull Compustat company names and build a long 'name_pool':\n",
    "    (gvkey, name_key, source_name, sic)\n",
    "\n",
    "    Adds filters to avoid funds/ETFs/trusts/etc.\n",
    "    \"\"\"\n",
    "    comp_cols = db.describe_table(library=\"comp\", table=\"company\")\n",
    "    comp_cols = [c.lower() for c in comp_cols[\"name\"].tolist()]\n",
    "\n",
    "    base_cols = [\"gvkey\", \"conm\", \"conml\", \"sic\"]\n",
    "    select_cols = [c for c in base_cols if c in comp_cols]\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT {\", \".join(select_cols)}\n",
    "    FROM comp.company\n",
    "    WHERE gvkey IS NOT NULL\n",
    "    \"\"\"\n",
    "    comp = db.raw_sql(sql)\n",
    "    comp.columns = [c.lower() for c in comp.columns]\n",
    "    comp[\"gvkey\"] = comp[\"gvkey\"].apply(gvkey6)\n",
    "\n",
    "    # ---- normalize SIC to numeric-ish string for filtering\n",
    "    if \"sic\" in comp.columns:\n",
    "        comp[\"sic\"] = pd.to_numeric(comp[\"sic\"], errors=\"coerce\")\n",
    "    else:\n",
    "        comp[\"sic\"] = pd.NA\n",
    "\n",
    "    # =========================================================\n",
    "    # 1) HARD FILTER: drop investment vehicles (SIC 67xx)\n",
    "    # =========================================================\n",
    "    # SIC 67xx includes investment offices, funds, trusts, ETFs, etc.\n",
    "    comp = comp[~comp[\"sic\"].between(6700, 6799, inclusive=\"both\") | comp[\"sic\"].isna()].copy()\n",
    "\n",
    "    # =========================================================\n",
    "    # 2) SOFT FILTER: drop names that look like funds/ETFs/trusts\n",
    "    # =========================================================\n",
    "    bad_name_re = re.compile(\n",
    "        r\"\\b(etf|funds?|trust|income|portfolio|index|notes?|\"\n",
    "        r\"depositary|adr|etn|closed[-\\s]?end|open[-\\s]?end|\"\n",
    "        r\"spac|acquisition\\s+corp|holdings?\\s+trust)\\b\",\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    for col in [\"conm\", \"conml\"]:\n",
    "        if col in comp.columns:\n",
    "            comp[col] = comp[col].fillna(\"\")\n",
    "            comp = comp[~comp[col].str.contains(bad_name_re, na=False)].copy()\n",
    "        else:\n",
    "            comp[col] = \"\"\n",
    "\n",
    "    # ---- build match keys\n",
    "    comp[\"conm_key\"]  = comp[\"conm\"].apply(make_match_key)  if \"conm\"  in comp.columns else \"\"\n",
    "    comp[\"conml_key\"] = comp[\"conml\"].apply(make_match_key) if \"conml\" in comp.columns else \"\"\n",
    "\n",
    "    name_pool = pd.concat(\n",
    "        [\n",
    "            comp[[\"gvkey\", \"conm\", \"conm_key\", \"sic\"]].rename(\n",
    "                columns={\"conm\": \"source_name\", \"conm_key\": \"name_key\"}\n",
    "            ),\n",
    "            comp[[\"gvkey\", \"conml\", \"conml_key\", \"sic\"]].rename(\n",
    "                columns={\"conml\": \"source_name\", \"conml_key\": \"name_key\"}\n",
    "            ),\n",
    "        ],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    name_pool = name_pool.dropna(subset=[\"gvkey\"])\n",
    "    name_pool[\"name_key\"] = name_pool[\"name_key\"].astype(str)\n",
    "    name_pool = name_pool[name_pool[\"name_key\"].str.len() > 0]\n",
    "    name_pool = name_pool.drop_duplicates(subset=[\"gvkey\", \"name_key\"])\n",
    "\n",
    "    return name_pool\n",
    "\n",
    "# -------------------------\n",
    "# 2) High-confidence matcher ONLY\n",
    "# -------------------------\n",
    "\n",
    "def match_gvkey_high_confidence(\n",
    "    df_names: pd.DataFrame,\n",
    "    name_pool: pd.DataFrame,\n",
    "    ok_cutoff: float = 92.0,\n",
    "    review_cutoff: float = 85.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Assign gvkey only when score >= ok_cutoff.\n",
    "    Keep REVIEW/NO_MATCH for transparency; but gvkey stays NA unless OK.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from rapidfuzz import process, fuzz\n",
    "    except Exception:\n",
    "        raise ImportError(\n",
    "            \"rapidfuzz is required. In Jupyter run: pip install rapidfuzz\"\n",
    "        )\n",
    "\n",
    "    choices = name_pool[\"name_key\"].tolist()\n",
    "\n",
    "    out_gvkey = []\n",
    "    out_matched_key = []\n",
    "    out_score = []\n",
    "    out_flag = []\n",
    "\n",
    "    for mk in df_names[\"match_key\"].fillna(\"\").astype(str):\n",
    "        if not mk:\n",
    "            out_gvkey.append(pd.NA)\n",
    "            out_matched_key.append(pd.NA)\n",
    "            out_score.append(0.0)\n",
    "            out_flag.append(\"NO_MATCH\")\n",
    "            continue\n",
    "\n",
    "        hit = process.extractOne(mk, choices, scorer=fuzz.token_set_ratio)\n",
    "        if hit is None:\n",
    "            out_gvkey.append(pd.NA)\n",
    "            out_matched_key.append(pd.NA)\n",
    "            out_score.append(0.0)\n",
    "            out_flag.append(\"NO_MATCH\")\n",
    "            continue\n",
    "\n",
    "        matched_key, score, idx = hit\n",
    "        score = float(score)\n",
    "\n",
    "        if score >= ok_cutoff:\n",
    "            gv = name_pool.iloc[idx][\"gvkey\"]\n",
    "            out_gvkey.append(gv)\n",
    "            out_flag.append(\"OK\")\n",
    "        elif score >= review_cutoff:\n",
    "            out_gvkey.append(pd.NA)  # IMPORTANT: do not assign\n",
    "            out_flag.append(\"REVIEW\")\n",
    "        else:\n",
    "            out_gvkey.append(pd.NA)  # IMPORTANT: do not assign\n",
    "            out_flag.append(\"NO_MATCH\")\n",
    "\n",
    "        out_matched_key.append(matched_key)\n",
    "        out_score.append(score)\n",
    "\n",
    "    df = df_names.copy()\n",
    "    df[\"gvkey\"] = out_gvkey\n",
    "    df[\"comp_name_key_matched\"] = out_matched_key\n",
    "    df[\"name_match_score\"] = out_score\n",
    "    df[\"match_flag\"] = out_flag\n",
    "\n",
    "    # bring SIC for OK matches (optional, but useful)\n",
    "    df = df.merge(\n",
    "        name_pool[[\"gvkey\", \"sic\"]].drop_duplicates(\"gvkey\"),\n",
    "        on=\"gvkey\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# 3) One-call function for a year (scrape -> high-confidence gvkey)\n",
    "# -------------------------\n",
    "\n",
    "def scrape_and_match_gvkey(url: str, db, name_pool: pd.DataFrame, ok_cutoff=92.0, review_cutoff=85.0):\n",
    "    df = scrape_gptw_from_url(url)\n",
    "    df[\"name_raw\"] = df[\"company_name\"]\n",
    "    df[\"match_key\"] = df[\"company_name\"].apply(make_match_key)\n",
    "\n",
    "    df = match_gvkey_high_confidence(\n",
    "        df,\n",
    "        name_pool=name_pool,\n",
    "        ok_cutoff=ok_cutoff,\n",
    "        review_cutoff=review_cutoff,\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bfec010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "db = wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a12b4575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 56070 rows in comp.company.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/390345803.py:136: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  comp = comp[~comp[col].str.contains(bad_name_re, na=False)].copy()\n",
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/390345803.py:136: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  comp = comp[~comp[col].str.contains(bad_name_re, na=False)].copy()\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# USAGE\n",
    "# -------------------------\n",
    "# Run ONCE:\n",
    "name_pool = build_compustat_name_pool(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "78367d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025 = scrape_and_match_gvkey(\"https://www.greatplacetowork.com/best-workplaces/100-best/2025\", db, name_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "15af952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we clean\n",
    "\n",
    "def clean_df(\n",
    "    df,\n",
    "    private_companies,\n",
    "    manual_gvkey_overrides,\n",
    "    validate=True,\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure 'public' exists\n",
    "    if \"public\" not in df.columns:\n",
    "        df[\"public\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Force private companies\n",
    "    # -------------------------\n",
    "    private_set = {str(x).upper().strip() for x in private_companies}\n",
    "\n",
    "    mask_private = (\n",
    "        df[\"company_name\"]\n",
    "        .astype(str)\n",
    "        .str.upper()\n",
    "        .str.strip()\n",
    "        .isin(private_set)\n",
    "    )\n",
    "\n",
    "    df.loc[mask_private, \"gvkey\"] = np.nan\n",
    "    df.loc[mask_private, \"public\"] = 0\n",
    "    df.loc[mask_private, \"match_flag\"] = \"MANUAL_PRIVATE\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Manual GVKEY overrides\n",
    "    # -------------------------\n",
    "    for name, vals in manual_gvkey_overrides.items():\n",
    "        nm = str(name).upper().strip()\n",
    "\n",
    "        mask = (\n",
    "            df[\"company_name\"]\n",
    "            .astype(str)\n",
    "            .str.upper()\n",
    "            .str.strip()\n",
    "            .eq(nm)\n",
    "        )\n",
    "\n",
    "        df.loc[mask, \"gvkey\"] = vals[\"gvkey\"]\n",
    "        if \"conm\" in df.columns and \"conm\" in vals:\n",
    "            df.loc[mask, \"conm\"] = vals[\"conm\"]\n",
    "\n",
    "        df.loc[mask, \"match_flag\"] = \"MANUAL_OVERRIDE\"\n",
    "        df.loc[mask, \"public\"] = 1  # overrides are meant to be public\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Final public flag (ONLY fill missing)\n",
    "    # -------------------------\n",
    "    mask_public_missing = df[\"public\"].isna()\n",
    "    df.loc[mask_public_missing, \"public\"] = np.where(\n",
    "        df.loc[mask_public_missing, \"gvkey\"].notna(), 1, 0\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Optional validation\n",
    "    # -------------------------\n",
    "    if validate:\n",
    "        bad_private = df.loc[\n",
    "            (df[\"public\"] == 0) & df[\"gvkey\"].notna(),\n",
    "            [\"company_name\", \"gvkey\", \"match_flag\"],\n",
    "        ]\n",
    "        bad_public = df.loc[\n",
    "            (df[\"public\"] == 1) & df[\"gvkey\"].isna(),\n",
    "            [\"company_name\", \"match_flag\"],\n",
    "        ]\n",
    "\n",
    "        if not bad_private.empty or not bad_public.empty:\n",
    "            raise ValueError(\n",
    "                \"Public/private consistency check failed.\\n\\n\"\n",
    "                f\"Private with GVKEY:\\n{bad_private}\\n\\n\"\n",
    "                f\"Public without GVKEY:\\n{bad_public}\"\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b0aa7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_gvkey_overrides = {\n",
    "    \"TARGET CORPORATION\": {\n",
    "        \"gvkey\": \"002572\",   # Target Corp (retail)\n",
    "        \"conm\": \"TARGET CORP\"\n",
    "    },\n",
    "    \"BOX, INC.\": {\n",
    "        \"gvkey\": \"012141\",   # Box Inc (cloud software)\n",
    "        \"conm\": \"BOX INC\"\n",
    "    },\n",
    "    \"DOW\": {\n",
    "        \"gvkey\": \"018257\",   # Dow Inc (ticker: DOW)\n",
    "        \"conm\": \"DOW INC\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f71549da",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_companies = [\n",
    "    \"Wegmans Food Markets, Inc.\",\n",
    "    \"World Wide Technology\",\n",
    "    \"David Weekley Homes\",\n",
    "    \"PricewaterhouseCoopers LLP\",\n",
    "    \"Plante Moran\",\n",
    "    \"Kimley-Horn\",\n",
    "    \"Deloitte\",\n",
    "    \"Nationwide Mutual Insurance Company\",\n",
    "    \"Bain & Company, Inc.\",\n",
    "    \"Sheetz, Inc.\",\n",
    "    \"OneDigital\",\n",
    "    \"Trek Bicycle\",\n",
    "    \"Panda Restaurant Group Inc.\",\n",
    "    \"EY\",\n",
    "    \"RSM US LLP\",\n",
    "    \"Baptist Health South Florida\",\n",
    "    \"BDO USA\",\n",
    "    \"KPMG LLP\",\n",
    "    \"Protiviti\",\n",
    "    \"The Wonderful Company\",\n",
    "    \"Allianz Life\",\n",
    "    \"SCAN Health Plan\",\n",
    "    \"Perkins Coie LLP\",\n",
    "    \"PCL Construction\",\n",
    "    \"TP (including Alliance One, Health Advocate, Language Line Solutions, PSG Global Solutions, Senture, and TLScontact)\",\n",
    "    \"Wellstar Health System\",\n",
    "    \"Hilti, Inc\",\n",
    "    \"Jackson Healthcare\",\n",
    "    \"Publix Super Markets\",\n",
    "    \"CHG Healthcare Services\",\n",
    "    \"Veterans United Home Loans\",\n",
    "    \"Atlantic Health System\",\n",
    "    \"Modern Technology Solutions, Inc.\",\n",
    "    \"Ryan, LLC\",\n",
    "    \"Alston & Bird LLP\",\n",
    "    \"Baird\",\n",
    "\n",
    "    # ðŸ”´ added now (previously leaking gvkeys)\n",
    "    \"Power Home Remodeling\",\n",
    "    \"DHL Express\",\n",
    "    \"IGS Energy\",\n",
    "    \"New American Funding\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_clean = clean_df(\n",
    "    df_2025,\n",
    "    private_companies=private_companies,\n",
    "    manual_gvkey_overrides=manual_gvkey_overrides\n",
    ")\n",
    "# Now let's sum the 1 in the public column: \n",
    "\n",
    "print(df_2025_clean[\"public\"].sum())\n",
    "\n",
    "df_2025_clean.to_excel(\"great_place_to_work_2025_final.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a0dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "956e3b9b",
   "metadata": {},
   "source": [
    "### Now we start building up for the previous years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8d35942",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_COLS = [\"match_key\", \"gvkey\", \"sic\", \"public\", \"company_name\"]\n",
    "\n",
    "def init_master_warm(df_clean):\n",
    "    \"\"\"\n",
    "    Start the master warm table from a cleaned year df.\n",
    "    Keeps only rows with a gvkey, one per match_key.\n",
    "    \"\"\"\n",
    "    cols = [c for c in WARM_COLS if c in df_clean.columns]\n",
    "    master = (\n",
    "        df_clean\n",
    "        .dropna(subset=[\"match_key\", \"gvkey\"])\n",
    "        .sort_values([\"match_key\", \"year\", \"rank\"])\n",
    "        .drop_duplicates(\"match_key\", keep=\"last\")\n",
    "        [cols]\n",
    "        .copy()\n",
    "    )\n",
    "    return master\n",
    "\n",
    "def update_master_warm(master_warm, df_new_clean):\n",
    "    \"\"\"\n",
    "    Add/overwrite match_key rows using the newest cleaned df.\n",
    "    New df wins when match_key conflicts.\n",
    "    \"\"\"\n",
    "    cols = [c for c in WARM_COLS if c in df_new_clean.columns]\n",
    "    add = (\n",
    "        df_new_clean\n",
    "        .dropna(subset=[\"match_key\", \"gvkey\"])\n",
    "        .sort_values([\"match_key\", \"year\", \"rank\"])\n",
    "        .drop_duplicates(\"match_key\", keep=\"last\")\n",
    "        [cols]\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    master = pd.concat([master_warm[cols], add], ignore_index=True)\n",
    "    master = master.drop_duplicates(\"match_key\", keep=\"last\")  # newest wins\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f354df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_warm_start_from_master(df_year_raw, master_warm):\n",
    "    \"\"\"\n",
    "    Fill gvkey/sic/public/conm for any match_key found in master_warm.\n",
    "    \"\"\"\n",
    "    df = df_year_raw.copy()\n",
    "\n",
    "    # ensure output cols exist\n",
    "    for c in [\"gvkey\", \"sic\", \"public\", \"match_flag\", \"company_name\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    warm_map = master_warm.set_index(\"match_key\").to_dict(orient=\"index\")\n",
    "\n",
    "    for i, mk in df[\"match_key\"].fillna(\"\").items():\n",
    "        if mk in warm_map:\n",
    "            for k, v in warm_map[mk].items():\n",
    "                df.at[i, k] = v\n",
    "            df.at[i, \"match_flag\"] = \"WARM_START_MASTER\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d7e53da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_year_cumulative(\n",
    "    url,\n",
    "    db,\n",
    "    name_pool,\n",
    "    master_warm,\n",
    "    private_companies,\n",
    "    manual_gvkey_overrides,\n",
    "    ok_cutoff=92.0,\n",
    "    review_cutoff=85.0,\n",
    "    validate=True,\n",
    "):\n",
    "    # 1) scrape\n",
    "    df = scrape_gptw_from_url(url)\n",
    "    df[\"name_raw\"] = df[\"company_name\"]\n",
    "    df[\"match_key\"] = df[\"company_name\"].apply(make_match_key)\n",
    "\n",
    "    # 2) warm start from ALL prior cleaned years\n",
    "    if master_warm is not None and len(master_warm) > 0:\n",
    "        df = apply_warm_start_from_master(df, master_warm)\n",
    "\n",
    "    # 3) force private + manual overrides BEFORE fuzzy\n",
    "    df = clean_df(\n",
    "        df,\n",
    "        private_companies=private_companies,\n",
    "        manual_gvkey_overrides=manual_gvkey_overrides,\n",
    "        validate=False,\n",
    "    )\n",
    "\n",
    "    # 4) fuzzy match only for rows still missing gvkey AND not forced private\n",
    "    needs = (df[\"gvkey\"].isna()) & (df[\"match_flag\"] != \"MANUAL_PRIVATE\")\n",
    "    if needs.any():\n",
    "        df_matched = match_gvkey_high_confidence(\n",
    "            df.loc[needs, [\"year\",\"rank\",\"company_name\",\"name_raw\",\"match_key\"]].copy(),\n",
    "            name_pool=name_pool,\n",
    "            ok_cutoff=ok_cutoff,\n",
    "            review_cutoff=review_cutoff,\n",
    "        )\n",
    "        for col in [\"gvkey\", \"sic\", \"comp_name_key_matched\", \"name_match_score\", \"match_flag\"]:\n",
    "            if col in df_matched.columns:\n",
    "                df.loc[needs, col] = df_matched[col].values\n",
    "\n",
    "    # 5) final public flag (donâ€™t override MANUAL_PRIVATE)\n",
    "    mask_pub_missing = df[\"public\"].isna()\n",
    "    df.loc[mask_pub_missing, \"public\"] = np.where(df.loc[mask_pub_missing, \"gvkey\"].notna(), 1, 0)\n",
    "\n",
    "    # 6) validate (optional)\n",
    "    if validate:\n",
    "        bad_private = df[(df[\"match_flag\"] == \"MANUAL_PRIVATE\") & df[\"gvkey\"].notna()]\n",
    "        if not bad_private.empty:\n",
    "            raise ValueError(\"Some MANUAL_PRIVATE rows still have gvkey assigned.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f2da8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/597450935.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '005643' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[i, k] = v\n",
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/597450935.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'WARM_START_MASTER' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[i, \"match_flag\"] = \"WARM_START_MASTER\"\n",
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/2998323198.py:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<IntegerArray>\n",
      "[<NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, 7373, <NA>, 6153,\n",
      " <NA>, 4911, 6163, 5000, <NA>, <NA>]\n",
      "Length: 19, dtype: Int64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[needs, col] = df_matched[col].values\n",
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/597450935.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '020779' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[i, k] = v\n",
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/597450935.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'WARM_START_MASTER' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[i, \"match_flag\"] = \"WARM_START_MASTER\"\n",
      "/var/folders/bb/rsmxjk4n2k7_166l026d8h400000gn/T/ipykernel_28414/2998323198.py:40: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<IntegerArray>\n",
      "[<NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, 7370, <NA>, 4220, <NA>, <NA>, <NA>,\n",
      " 7372, 7370, 6552, 6311, 2300, <NA>, <NA>]\n",
      "Length: 20, dtype: Int64' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[needs, col] = df_matched[col].values\n"
     ]
    }
   ],
   "source": [
    "# start with 2025 (already cleaned)\n",
    "master_warm = init_master_warm(df_2025_clean)\n",
    "\n",
    "# run 2024\n",
    "df_2024_clean = run_year_cumulative(\n",
    "    \"https://www.greatplacetowork.com/best-workplaces/100-best/2024\",\n",
    "    db=db,\n",
    "    name_pool=name_pool,\n",
    "    master_warm=master_warm,\n",
    "    private_companies=private_companies,\n",
    "    manual_gvkey_overrides=manual_gvkey_overrides,\n",
    ")\n",
    "\n",
    "# update master with 2024 results\n",
    "master_warm = update_master_warm(master_warm, df_2024_clean)\n",
    "\n",
    "# run 2023 (now warm-starts from BOTH 2025 + 2024)\n",
    "df_2023_clean = run_year_cumulative(\n",
    "    \"https://www.greatplacetowork.com/best-workplaces/100-best/2023\",\n",
    "    db=db,\n",
    "    name_pool=name_pool,\n",
    "    master_warm=master_warm,\n",
    "    private_companies=private_companies,\n",
    "    manual_gvkey_overrides=manual_gvkey_overrides,\n",
    ")\n",
    "\n",
    "master_warm = update_master_warm(master_warm, df_2023_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ae41bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we save them to excel \n",
    "\n",
    "df_2024_clean.to_excel(\"great_place_to_work_2024_final.xlsx\", index=False)\n",
    "df_2023_clean.to_excel(\"great_place_to_work_2023_final.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ba60a4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2024_clean[\"gvkey\"].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "10828c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2023_clean[\"gvkey\"].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e9a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
